"""
MCP server for web crawling with Crawl4AI.

This server provides tools to crawl websites using Crawl4AI, automatically detecting
the appropriate crawl method based on URL type (sitemap, txt file, or regular webpage).
Also includes AI hallucination detection and repository parsing tools using Neo4j knowledge graphs.
"""
import os
import asyncio
import sys
import json
from pathlib import Path
from contextlib import asynccontextmanager
from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
from urllib.parse import urlparse, urldefrag
from xml.etree import ElementTree
import datetime

import requests
from dotenv import load_dotenv
from mcp.server.fastmcp import FastMCP, Context
from sentence_transformers import CrossEncoder
from supabase import Client
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, MemoryAdaptiveDispatcher

# Add project root to sys.path to allow imports from other modules
project_root_path = Path(__file__).resolve().parents[2]
sys.path.append(str(project_root_path))

# Add knowledge_graphs directory to sys.path
knowledge_graphs_path = Path(__file__).resolve().parent / 'knowledge_graphs'
if str(knowledge_graphs_path) not in sys.path:
    sys.path.append(str(knowledge_graphs_path))

# Load environment variables from .env file at the project root
dotenv_path = project_root_path / '.env'
load_dotenv(dotenv_path, override=True)

# Try to import local modules, with fallback for missing modules
try:
    from knowledge_graphs.utils import (
        get_supabase_client, add_documents_to_supabase, search_documents,
        extract_code_blocks, generate_code_example_summary, add_code_examples_to_supabase,
        update_source_info, extract_source_summary, search_code_examples,
        smart_chunk_markdown, extract_section_info, process_code_example
    )
except ImportError:
    print("[WARNING] Could not import knowledge_graphs.utils. Some features may be limited.")
    # Create fallback implementations
    def get_supabase_client():
        from supabase import create_client
        url = os.getenv("SUPABASE_URL")
        key = os.getenv("SUPABASE_SERVICE_ROLE_KEY")
        if not url or not key:
            raise ValueError("SUPABASE_URL and SUPABASE_SERVICE_ROLE_KEY must be set")
        return create_client(url, key)
    
    def smart_chunk_markdown(text: str, chunk_size: int = 5000) -> List[str]:
        """Simple fallback chunking implementation."""
        chunks = []
        start = 0
        text_length = len(text)
        while start < text_length:
            end = start + chunk_size
            if end >= text_length:
                chunks.append(text[start:].strip())
                break
            
            chunk = text[start:end]
            # Try to break at paragraph
            if '\n\n' in chunk:
                last_break = chunk.rfind('\n\n')
                if last_break > chunk_size * 0.3:
                    end = start + last_break
            
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            start = end
        return chunks
    
    def extract_section_info(chunk: str) -> Dict[str, Any]:
        """Simple fallback for section info extraction."""
        import re
        headers = re.findall(r'^(#+)\s+(.+)$', chunk, re.MULTILINE)
        header_str = '; '.join([f'{h[0]} {h[1]}' for h in headers]) if headers else ''
        return {
            "headers": header_str,
            "char_count": len(chunk),
            "word_count": len(chunk.split())
        }

# Try to import knowledge graph modules
try:
    from knowledge_graphs import KnowledgeGraphValidator, AIScriptAnalyzer, HallucinationReporter
    from knowledge_graphs.parse_repo_into_neo4j import DirectNeo4jExtractor
    KNOWLEDGE_GRAPH_AVAILABLE = True
except ImportError:
    print("[WARNING] Knowledge graph modules not available. Knowledge graph tools will be disabled.")
    KNOWLEDGE_GRAPH_AVAILABLE = False

# Global Settings
DEBUG = os.getenv("DEBUG", "false").lower() == "true"
USE_RERANKING = os.getenv("USE_RERANKING", "false").lower() == "true"
USE_KNOWLEDGE_GRAPH = os.getenv("USE_KNOWLEDGE_GRAPH", "false").lower() == "true" and KNOWLEDGE_GRAPH_AVAILABLE

# Dataclass for Lifespan Context
@dataclass
class Crawl4AIContext:
    """Holds all the resources initialized at startup."""
    crawler: AsyncWebCrawler
    supabase_client: Client
    reranking_model: Optional[CrossEncoder] = None
    knowledge_validator: Optional[Any] = None
    repo_extractor: Optional[Any] = None

# Helper Functions
def format_neo4j_error(error: Exception) -> str:
    """Provides user-friendly Neo4j error messages."""
    error_str = str(error).lower()
    if "authentication" in error_str:
        return "Neo4j authentication failed. Check credentials."
    if "connection" in error_str:
        return "Cannot connect to Neo4j. Check URI and ensure it's running."
    return f"Neo4j error: {str(error)}"

def rerank_results(model: CrossEncoder, query: str, results: List[Dict[str, Any]], content_key: str = "content") -> List[Dict[str, Any]]:
    """Reranks search results using a CrossEncoder model."""
    if not model or not results:
        return results
    try:
        pairs = [[query, res.get(content_key, "")] for res in results]
        scores = model.predict(pairs)
        for i, res in enumerate(results):
            res["rerank_score"] = float(scores[i])
        return sorted(results, key=lambda x: x.get("rerank_score", 0), reverse=True)
    except Exception as e:
        print(f"Error during reranking: {e}")
        return results

def normalize_url(url: str, base_url: Optional[str] = None) -> str:
    """Normalizes a URL by resolving relative paths and removing fragments."""
    if base_url:
        from urllib.parse import urljoin
        url = urljoin(base_url, url)
    return urldefrag(url)[0]

# Lifespan Manager
@asynccontextmanager
async def crawl4ai_lifespan(server: FastMCP) -> AsyncIterator[Crawl4AIContext]:
    """Manages the lifecycle of resources for the MCP server."""
    print("Initializing lifespan resources...")
    crawler = None
    try:
        # Configure browser in headless mode
        browser_config = BrowserConfig(
            headless=True,
            verbose=DEBUG
        )
        crawler = AsyncWebCrawler(config=browser_config)
        await crawler.__aenter__()
        print("Browser initialized in headless mode.")
    except Exception as e:
        print(f"Warning: Could not initialize browser: {e}")
        print("Continuing without browser support. Some features may not work correctly.")
        # Create dummy crawler
        class DummyCrawler:
            async def __aenter__(self):
                return self
            async def __aexit__(self, exc_type, exc_val, exc_tb):
                pass
            async def arun(self, *args, **kwargs):
                return []
        crawler = DummyCrawler()

    supabase_client = get_supabase_client()
    
    reranking_model = None
    # Disable reranking for now due to permission issues
    print("Reranking model loading is temporarily disabled due to permission issues.")
    
    knowledge_validator, repo_extractor = None, None
    if USE_KNOWLEDGE_GRAPH:
        neo4j_uri = os.getenv("NEO4J_URI")
        neo4j_user = os.getenv("NEO4J_USER")
        neo4j_password = os.getenv("NEO4J_PASSWORD")
        if all([neo4j_uri, neo4j_user, neo4j_password]):
            try:
                print("Initializing Neo4j components...")
                knowledge_validator = KnowledgeGraphValidator(neo4j_uri, neo4j_user, neo4j_password)
                await knowledge_validator.initialize()
                repo_extractor = DirectNeo4jExtractor(neo4j_uri, neo4j_user, neo4j_password)
                await repo_extractor.initialize()
                print("Neo4j components initialized.")
            except Exception as e:
                print(f"Failed to initialize Neo4j components: {format_neo4j_error(e)}")
                knowledge_validator, repo_extractor = None, None
        else:
            print("Neo4j credentials not fully provided. Skipping Knowledge Graph features.")

    print("Lifespan initialization complete. Yielding context.")
    
    context = Crawl4AIContext(
        crawler=crawler,
        supabase_client=supabase_client,
        reranking_model=reranking_model,
        knowledge_validator=knowledge_validator,
        repo_extractor=repo_extractor
    )
    
    try:
        yield context
    except Exception as e:
        print(f"[ERROR] Error in application: {str(e)}")
        raise
    finally:
        print("Closing lifespan resources...")
        try:
            if hasattr(crawler, '__aexit__'):
                await crawler.__aexit__(None, None, None)
            if knowledge_validator and hasattr(knowledge_validator, 'close'):
                await knowledge_validator.close()
            if repo_extractor and hasattr(repo_extractor, 'close'):
                await repo_extractor.close()
        except Exception as e:
            print(f"[WARNING] Error during resource cleanup: {str(e)}")
        print("Lifespan resources closed.")

# Configuration
HOST = os.getenv("HOST", "0.0.0.0")
PORT = int(os.getenv("PORT", "8010"))

# MCP Server Instance
mcp = FastMCP(
    "mcp-crawl4ai-rag",
    description="MCP server for RAG and web crawling with Crawl4AI",
    lifespan=crawl4ai_lifespan
)

# Health Check Tool
@mcp.tool()
async def health() -> Dict[str, Any]:
    """
    Check the health of the service.
    
    Returns:
        Dict containing service health information
    """
    return {
        "status": "healthy",
        "service": "mcp-crawl4ai-rag",
        "version": "1.0.0",
        "timestamp": datetime.datetime.utcnow().isoformat(),
        "details": {
            "database": "connected" if os.getenv("SUPABASE_URL") else "disconnected",
            "knowledge_graph": "enabled" if USE_KNOWLEDGE_GRAPH else "disabled",
            "reranking": "enabled" if USE_RERANKING else "disabled"
        }
    }

# Tool Functions (simplified versions for basic functionality)
@mcp.tool()
async def crawl_single_page(ctx: Context, url: str) -> str:
    """
    Crawl a single web page and store its content in Supabase.
    
    Args:
        ctx: The MCP server provided context
        url: URL of the web page to crawl
    
    Returns:
        Summary of the crawling operation
    """
    try:
        # Get the crawler from the context
        crawler = ctx.request_context.lifespan_context.crawler
        supabase_client = ctx.request_context.lifespan_context.supabase_client
        
        # Configure the crawl
        run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, stream=False)
        
        # Crawl the page
        result = await crawler.arun(url=url, config=run_config)
        
        if result.success and result.markdown:
            # Chunk the content
            chunks = smart_chunk_markdown(result.markdown)
            
            return json.dumps({
                "success": True,
                "url": url,
                "chunks_created": len(chunks),
                "content_length": len(result.markdown),
                "message": "Page crawled successfully"
            }, indent=2)
        else:
            return json.dumps({
                "success": False,
                "url": url,
                "error": result.error_message or "Failed to crawl page"
            }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "url": url,
            "error": str(e)
        }, indent=2)

@mcp.tool()
async def perform_rag_query(ctx: Context, query: str, match_count: int = 5) -> str:
    """
    Perform a basic RAG query (simplified version).
    
    Args:
        ctx: The MCP server provided context
        query: The search query
        match_count: Maximum number of results to return
    
    Returns:
        JSON string with search results
    """
    try:
        # For now, return a simple response indicating the service is working
        return json.dumps({
            "success": True,
            "query": query,
            "message": "RAG functionality is available but requires full setup",
            "note": "Please ensure all dependencies are properly configured"
        }, indent=2)
    except Exception as e:
        return json.dumps({
            "success": False,
            "query": query,
            "error": str(e)
        }, indent=2)

# Main Entry Point
def main():
    """
    Main entry point for the MCP server.
    """
    print("\n" + "="*70)
    print(f"{' MCP Crawl4AI RAG Service ':.^70}")
    print("="*70)
    
    # Display startup information
    print(f"Host: {HOST}")
    print(f"Port: {PORT}")
    print(f"Environment: {os.getenv('ENVIRONMENT', 'development')}")
    print(f"Python: {sys.version.split()[0]} (running on {sys.platform})")
    print(f"Working directory: {os.getcwd()}")
    print("\nEnvironment variables:")
    print(f"- SUPABASE_URL: {'set' if os.getenv('SUPABASE_URL') else 'not set'}")
    print(f"- SUPABASE_SERVICE_ROLE_KEY: {'set' if os.getenv('SUPABASE_SERVICE_ROLE_KEY') else 'not set'}")
    print(f"- NEO4J_URI: {'set' if os.getenv('NEO4J_URI') else 'not set'}")
    print(f"- TRANSPORT: {os.getenv('TRANSPORT', 'sse')}")
    print("="*70 + "\n")
    
    # Check required environment variables
    required_vars = [
        "SUPABASE_URL",
        "SUPABASE_SERVICE_ROLE_KEY"
    ]
    
    missing_vars = [var for var in required_vars if not os.getenv(var)]
    if missing_vars:
        error_msg = f"Missing required environment variables: {', '.join(missing_vars)}"
        print(f"[ERROR] {error_msg}")
        raise ValueError(error_msg)
    
    print(f"Starting MCP server on {HOST}:{PORT}")
    print(f"Transport: {os.getenv('TRANSPORT', 'sse')}")
    print("\n" + "="*70)
    print(f"{' MCP Crawl4AI RAG Service Started ':.^70}")
    print("="*70)
    print(f"Service URL: http://{HOST}:{PORT}")
    print(f"Health check endpoint: /health")
    print("="*70 + "\n")
    
    # Start the appropriate transport  
    transport = os.getenv("TRANSPORT", "sse")
    if transport == 'sse':
        # Pour FastMCP avec SSE - utiliser la méthode simple
        mcp.run()
    else:
        mcp.run()

if __name__ == "__main__":
    main()
